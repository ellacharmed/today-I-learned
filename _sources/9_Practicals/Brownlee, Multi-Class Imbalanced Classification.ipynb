{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Class Imbalanced Classification\n",
    "~ Last Updated on January 5, 2021 <br/>\n",
    "~ Jason Brownlee\n",
    "\n",
    "source: https://machinelearningmastery.com/multi-class-imbalanced-classification/\n",
    "\n",
    "Imbalanced classification are those prediction tasks where the distribution of examples across class labels is not equal.\n",
    "\n",
    "Most imbalanced classification examples focus on binary classification tasks, yet many of the tools and techniques for imbalanced classification also directly support multi-class classification problems.\n",
    "\n",
    "In this tutorial, you will discover how to use the tools of imbalanced classification with a multi-class dataset.\n",
    "\n",
    "After completing this tutorial, you will know:\n",
    "\n",
    "* About the glass identification standard imbalanced multi-class prediction problem.\n",
    "* How to use SMOTE oversampling for imbalanced multi-class classification.\n",
    "* How to use cost-sensitive learning for imbalanced multi-class classification.\n",
    "\n",
    "\n",
    "## Tutorial Overview\n",
    "\n",
    "This tutorial is divided into three parts; they are:\n",
    "\n",
    "1. Glass Multi-Class Classification Dataset\n",
    "1. SMOTE Oversampling for Multi-Class Classification\n",
    "1. Cost-Sensitive Learning for Multi-Class Classification\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from pathlib import Path\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "# check version number\n",
    "import imblearn\n",
    "print(f\"{imblearn.__version__ = }\")\n",
    "\n",
    "from pandas import read_csv\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup local paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_curr = Path.cwd()\n",
    "print(f'{path_curr = }')\n",
    "parent = path_curr.parent\n",
    "print(f'{parent = }')\n",
    "path_data = parent / 'data'\n",
    "path_images = parent / 'images'\n",
    "path_src = parent / 'src'\n",
    "\n",
    "if not path_data.exists():\n",
    "    # Path(path_images).mkdir(parents=True, exist_ok=True)\n",
    "    Path(path_data).mkdir(parents=True, exist_ok=True)\n",
    "    # Path(path_src).mkdir(parents=True, exist_ok=True)\n",
    "    print(f'subfolders did not exist, have been created')\n",
    "    \n",
    "print(f'{path_images = }')\n",
    "print(f'{path_data = }')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/glass.csv'\n",
    "date_desc = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/glass.names'\n",
    "\n",
    "path_glass = path_data / 'glass.csv'\n",
    "\n",
    "urlretrieve(data_url, path_glass)\n",
    "urlretrieve(date_desc, path_data / 'glass_desc.txt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load and summarize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the dataset location\n",
    "# url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/glass.csv'\n",
    "# load the csv file as a data frame\n",
    "df = read_csv(path_glass, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.values\n",
    "# split into input and output elements\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "# label encode the target variable\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "# summarize distribution\n",
    "counter = Counter(y)\n",
    "for k,v in counter.items():\n",
    "\tper = v / len(y) * 100\n",
    "\tprint('Class=%d, n=%d (%.3f%%)' % (k, v, per))\n",
    "# plot the distribution\n",
    "pyplot.bar(counter.keys(), counter.values())\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of oversampling a multi-class classification dataset\n",
    "\n",
    "# define the dataset location\n",
    "# url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/glass.csv'\n",
    "# load the csv file as a data frame\n",
    "# df = read_csv(url, header=None)\n",
    "# data = df.values\n",
    "# split into input and output elements\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "# label encode the target variable\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "# transform the dataset\n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(X, y)\n",
    "# summarize distribution\n",
    "counter = Counter(y)\n",
    "for k,v in counter.items():\n",
    "\tper = v / len(y) * 100\n",
    "\tprint('Class=%d, n=%d (%.3f%%)' % (k, v, per))\n",
    "# plot the distribution\n",
    "pyplot.bar(counter.keys(), counter.values())\n",
    "pyplot.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using the default strategy of SMOTE to oversample all classes to the number of examples in the majority class, we could instead specify the number of examples to oversample in each class.\n",
    "\n",
    "For example, we could oversample to 100 examples in classes 0 and 1 and 200 examples in remaining classes. This can be achieved by creating a dictionary that maps class labels to the number of desired examples in each class, then specifying this via the “sampling_strategy” argument to the SMOTE class.\n",
    "\n",
    "```...\n",
    "# transform the dataset\n",
    "strategy = {0:100, 1:100, 2:200, 3:200, 4:200, 5:200}\n",
    "oversample = SMOTE(sampling_strategy=strategy)\n",
    "X, y = oversample.fit_resample(X, y)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of oversampling a multi-class classification dataset with a custom strategy\n",
    "# from pandas import read_csv\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# from collections import Counter\n",
    "# from matplotlib import pyplot\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# # define the dataset location\n",
    "# url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/glass.csv'\n",
    "# # load the csv file as a data frame\n",
    "# df = read_csv(url, header=None)\n",
    "# data = df.values \n",
    "# split into input and output elements\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "# label encode the target variable\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "# transform the dataset\n",
    "strategy = {0:100, 1:100, 2:200, 3:200, 4:200, 5:200}\n",
    "oversample = SMOTE(sampling_strategy=strategy)\n",
    "X, y = oversample.fit_resample(X, y)\n",
    "# summarize distribution\n",
    "counter = Counter(y)\n",
    "for k,v in counter.items():\n",
    "\tper = v / len(y) * 100\n",
    "\tprint('Class=%d, n=%d (%.3f%%)' % (k, v, per))\n",
    "# plot the distribution\n",
    "pyplot.bar(counter.keys(), counter.values())\n",
    "pyplot.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: when using data sampling like SMOTE, it must only be applied to the training dataset, not the entire dataset. I recommend using a Pipeline to ensure that the SMOTE method is correctly used when evaluating models and making predictions with models.\n",
    "\n",
    "## Cost-Sensitive Learning for Multi-Class Classification\n",
    "\n",
    "Most machine learning algorithms assume that all classes have an equal number of examples.\n",
    "\n",
    "This is not the case in multi-class imbalanced classification. Algorithms can be modified to change the way learning is performed to bias towards those classes that have fewer examples in the training dataset. This is generally called cost-sensitive learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline model and test harness for the glass identification dataset\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# load the dataset\n",
    "def load_dataset(full_path):\n",
    "\t# load the dataset as a numpy array\n",
    "\tdata = read_csv(full_path, header=None)\n",
    "\t# retrieve numpy array\n",
    "\tdata = data.values\n",
    "\t# split into input and output elements\n",
    "\tX, y = data[:, :-1], data[:, -1]\n",
    "\t# label encode the target variable to have the classes 0 and 1\n",
    "\ty = LabelEncoder().fit_transform(y)\n",
    "\treturn X, y\n",
    "\n",
    "# evaluate a model\n",
    "def evaluate_model(X, y, model):\n",
    "\t# define evaluation procedure\n",
    "\tcv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "\t# evaluate model\n",
    "\tscores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\treturn scores\n",
    "\n",
    "# define the location of the dataset\n",
    "# full_path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/glass.csv'\n",
    "# load the dataset\n",
    "X, y = load_dataset(path_glass)\n",
    "# define the reference model\n",
    "model = RandomForestClassifier(n_estimators=1000)\n",
    "# evaluate the model\n",
    "scores = evaluate_model(X, y, model)\n",
    "# summarize performance\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can specify the “class_weight” argument to the value “balanced” that will automatically calculates a class weighting that will ensure each class gets an equal weighting during the training of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost sensitive random forest with default class weights\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# load the dataset\n",
    "def load_dataset(full_path):\n",
    "\t# load the dataset as a numpy array\n",
    "\tdata = read_csv(full_path, header=None)\n",
    "\t# retrieve numpy array\n",
    "\tdata = data.values\n",
    "\t# split into input and output elements\n",
    "\tX, y = data[:, :-1], data[:, -1]\n",
    "\t# label encode the target variable\n",
    "\ty = LabelEncoder().fit_transform(y)\n",
    "\treturn X, y\n",
    "\n",
    "# evaluate a model\n",
    "def evaluate_model(X, y, model):\n",
    "\t# define evaluation procedure\n",
    "\tcv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "\t# evaluate model\n",
    "\tscores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\treturn scores\n",
    "\n",
    "# define the location of the dataset\n",
    "# full_path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/glass.csv'\n",
    "# load the dataset\n",
    "X, y = load_dataset(path_glass)\n",
    "# define the model\n",
    "model = RandomForestClassifier(n_estimators=1000, class_weight='balanced')\n",
    "# evaluate the model\n",
    "scores = evaluate_model(X, y, model)\n",
    "# summarize performance\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The “class_weight” argument takes a dictionary of class labels mapped to a class weighting value.\n",
    "\n",
    "We can use this to specify a custom weighting, such as a default weighting for classes 0 and 1.0 that have many examples and a double class weighting of 2.0 for the other classes.\n",
    "\n",
    "```...\n",
    "# define the model\n",
    "weights = {0:1.0, 1:1.0, 2:2.0, 3:2.0, 4:2.0, 5:2.0}\n",
    "model = RandomForestClassifier(n_estimators=1000, class_weight=weights)\n",
    "```\n",
    "\n",
    "Tying this together, the complete example of using a custom class weighting for cost-sensitive learning on the glass multi-class imbalanced classification problem is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost sensitive random forest with custom class weightings\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# load the dataset\n",
    "def load_dataset(full_path):\n",
    "\t# load the dataset as a numpy array\n",
    "\tdata = read_csv(full_path, header=None)\n",
    "\t# retrieve numpy array\n",
    "\tdata = data.values\n",
    "\t# split into input and output elements\n",
    "\tX, y = data[:, :-1], data[:, -1]\n",
    "\t# label encode the target variable\n",
    "\ty = LabelEncoder().fit_transform(y)\n",
    "\treturn X, y\n",
    "\n",
    "# evaluate a model\n",
    "def evaluate_model(X, y, model):\n",
    "\t# define evaluation procedure\n",
    "\tcv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "\t# evaluate model\n",
    "\tscores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\treturn scores\n",
    "\n",
    "# define the location of the dataset\n",
    "# full_path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/glass.csv'\n",
    "# load the dataset\n",
    "X, y = load_dataset(path_glass)\n",
    "# define the model\n",
    "weights = {0:1.0, 1:1.0, 2:2.0, 3:2.0, 4:2.0, 5:2.0}\n",
    "model = RandomForestClassifier(n_estimators=1000, class_weight=weights)\n",
    "# evaluate the model\n",
    "scores = evaluate_model(X, y, model)\n",
    "# summarize performance\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
